{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/kmeulen/virtualenv/lib/python2.7/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: bokeh in /home/kmeulen/virtualenv/lib/python2.7/site-packages\n",
      "Requirement already satisfied: numpy>=1.7.1 in /home/kmeulen/virtualenv/lib/python2.7/site-packages (from bokeh)\n",
      "Requirement already satisfied: six>=1.5.2 in /home/kmeulen/virtualenv/lib/python2.7/site-packages (from bokeh)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/kmeulen/virtualenv/lib/python2.7/site-packages (from bokeh)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /home/kmeulen/virtualenv/lib/python2.7/site-packages (from bokeh)\n",
      "Requirement already satisfied: futures>=3.0.3 in /home/kmeulen/virtualenv/lib/python2.7/site-packages (from bokeh)\n",
      "Requirement already satisfied: Jinja2>=2.7 in /home/kmeulen/virtualenv/lib/python2.7/site-packages (from bokeh)\n",
      "Requirement already satisfied: packaging>=16.8 in /home/kmeulen/virtualenv/lib/python2.7/site-packages (from bokeh)\n",
      "Requirement already satisfied: tornado>=4.3 in /home/kmeulen/virtualenv/lib/python2.7/site-packages (from bokeh)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/kmeulen/virtualenv/lib/python2.7/site-packages (from Jinja2>=2.7->bokeh)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/kmeulen/virtualenv/lib/python2.7/site-packages (from packaging>=16.8->bokeh)\n",
      "Requirement already satisfied: singledispatch in /home/kmeulen/virtualenv/lib/python2.7/site-packages (from tornado>=4.3->bokeh)\n",
      "Requirement already satisfied: certifi in /home/kmeulen/virtualenv/lib/python2.7/site-packages (from tornado>=4.3->bokeh)\n",
      "Requirement already satisfied: backports-abc>=0.4 in /home/kmeulen/virtualenv/lib/python2.7/site-packages (from tornado>=4.3->bokeh)\n",
      "Requirement already satisfied: backports.ssl-match-hostname in /home/kmeulen/virtualenv/lib/python2.7/site-packages (from tornado>=4.3->bokeh)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"32c5777b-2e7d-44c2-b294-f72384273b1a\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"32c5777b-2e7d-44c2-b294-f72384273b1a\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"32c5777b-2e7d-44c2-b294-f72384273b1a\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '32c5777b-2e7d-44c2-b294-f72384273b1a' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.13.0.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"32c5777b-2e7d-44c2-b294-f72384273b1a\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"32c5777b-2e7d-44c2-b294-f72384273b1a\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"32c5777b-2e7d-44c2-b294-f72384273b1a\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '32c5777b-2e7d-44c2-b294-f72384273b1a' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.13.0.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.13.0.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.13.0.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.13.0.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"32c5777b-2e7d-44c2-b294-f72384273b1a\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import io, os, sys, types\n",
    "\n",
    "# Imports own outside scripts\n",
    "import Databaseloader as dl\n",
    "import loadinvizerfit as lif\n",
    "import jsonwriter\n",
    "import readandplotfits as rpf\n",
    "import writereg as wr\n",
    "import dbtools\n",
    "import cutout as ct\n",
    "#########################################\n",
    "import json\n",
    "import glob\n",
    "import matplotlib\n",
    "import loadjson as ljs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tkp.db\n",
    "from astropy.coordinates import SkyCoord\n",
    "from tkp.db.model import Varmetric\n",
    "from tkp.db.model import Runningcatalog\n",
    "from tkp.db.model import Newsource\n",
    "from tkp.db.model import Extractedsource\n",
    "from tkp.db.model import Image\n",
    "import operator\n",
    "from operator import itemgetter, attrgetter\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import sqlalchemy\n",
    "from sqlalchemy import *\n",
    "from sqlalchemy.orm import relationship\n",
    "#import Tools\n",
    "#import generic_tools\n",
    "#import plotting_tools\n",
    "from scipy.stats import norm\n",
    "import pylab\n",
    "pylab.rcParams['legend.loc'] = 'best'\n",
    "from matplotlib.ticker import NullFormatter\n",
    "from matplotlib.font_manager import FontProperties\n",
    "#from astroML import density_estimation\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy.spatial import distance\n",
    "import scipy.stats as ss\n",
    "import scipy.optimize as op\n",
    "import scipy.integrate as si\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install bokeh\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "from bokeh.models import DatetimeTickFormatter\n",
    "from bokeh.models import ColumnDataSource, Whisker\n",
    "from bokeh.io import export_png\n",
    "output_notebook()\n",
    "matplotlib.rcParams.update({'errorbar.capsize': 2})\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = 'postgresql'\n",
    "host = 'vlo.science.uva.nl'\n",
    "port = 5432\n",
    "user = 'kmeulen'\n",
    "password = 'kLu2oepRouv2UfoUPhoU'\n",
    "database='KmeulenSimSource'\n",
    "websiteURL = 'http://banana.transientskp.org/r4/vlo_'+database+'/runningcatalog/'\n",
    "latexURL = '\\url{http://banana.transientskp.org/r4/vlo_'+database+'/runningcatalog/'\n",
    "latexHREF = '\\href{http://banana.transientskp.org/r4/vlo_'+database+'/runningcatalog/'\n",
    "query_loglevel = logging.WARNING  # Set to INFO to see queries, otherwise WARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tkp.db.database:Database config: postgresql://kmeulen@vlo.science.uva.nl:5432/KmeulenSimSource\n",
      "/home/kmeulen/virtualenv/local/lib/python2.7/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n",
      "INFO:tkp.db.database:connecting to database...\n",
      "INFO:tkp.db.database:connected to: postgresql://kmeulen@vlo.science.uva.nl:5432/KmeulenSimSource\n"
     ]
    }
   ],
   "source": [
    "# this is sqlalchemy script to login to the Banana database\n",
    "logging.getLogger('sqlalchemy.engine').setLevel(query_loglevel)\n",
    "db = tkp.db.Database(engine=engine, host=host, port=port,\n",
    "                     user=user, password=password, database=database)\n",
    "db.connect()\n",
    "session = db.Session()\n",
    "\n",
    "# Here i get the peak flux and the error on the peak flux from all the sources in the database\n",
    "fpeak = session.query(Extractedsource.f_peak).all()\n",
    "fpeake = session.query(Extractedsource.f_peak_err).all()\n",
    "califreq = np.load(\"freqlist.npy\")\n",
    "califlux = np.load(\"fluxlist.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query to find the variability parameters of the sources in a specific dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tkp.db.database:Not configuring pre-configured database\n",
      "INFO:tkp.db.database:connecting to database...\n",
      "INFO:tkp.db.database:connected to: postgresql://kmeulen@vlo.science.uva.nl:5432/KmeulenSimSource\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected!\n"
     ]
    }
   ],
   "source": [
    "# Here I specifiy which dataset of the database to use\n",
    "dataset_id = 1\n",
    "timescale = \"1hr\"\n",
    "technique = database\n",
    "savefigs = True\n",
    "# runcat_id = 16077\n",
    "\n",
    "# VarParams = session.query(Varmetric,Runningcatalog).select_from(join(Varmetric,Runningcatalog)).filter(Runningcatalog.dataset_id == dataset_id).all()\n",
    "# imagecontrol = session.query(Runningcatalog,Image).filter(Runningcatalog.dataset_id == dataset_id).all()\n",
    "session = dbtools.access(engine,host,port,user,password,database)\n",
    "tables = ['image','varmetric','newsource']\n",
    "PandasParams = dbtools.GetPandaExtracted(session,dataset_id,tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print len(VarParams)\n",
    "# print len(imagecontrol)\n",
    "# for i in imagecontrol:\n",
    "#     print i\n",
    "# print PandasParams.keys()\n",
    "newPandas = PandasParams.drop_duplicates(subset='xtrsrc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make 2d array of all the sources except for the transients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "# plotdata = [[VarParams[i].Runningcatalog.id, VarParams[i].Runningcatalog.wm_ra,\\\n",
    "#              VarParams[i].Runningcatalog.wm_decl,VarParams[i].Runningcatalog.avg_ra_err,\\\n",
    "#              VarParams[i].Runningcatalog.avg_decl_err, VarParams[i].Runningcatalog.datapoints,\\\n",
    "#              session.query(Newsource).filter(Newsource.id==VarParams[i].Varmetric.newsource).one().newsource_type,\\\n",
    "#              VarParams[i].Varmetric.newsource,VarParams[i].Varmetric.lightcurve_max,\n",
    "#              session.query(Newsource).filter(Newsource.id==VarParams[i].Varmetric.newsource).one().trigger_xtrsrc.id,VarParams[i].Varmetric.eta_int]\\\n",
    "#             for i in range(len(VarParams)) if VarParams[i].Varmetric.newsource != None]\n",
    "# for i in plotdata:\n",
    "#     if i[0] ==2877:\n",
    "#         print i\n",
    "print len(newPandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdata = [[i.runcat, i.wm_ra,\\\n",
    "             i.wm_decl,i.avg_ra_err,\\\n",
    "             i.avg_decl_err, i.datapoints,\\\n",
    "             i.newsource_type,\\\n",
    "             i.newsource,i.lightcurve_max,\n",
    "             i.xtrsrc,i.eta_int,i.url]\\\n",
    "            for index,i in newPandas.iterrows() if i.newsource != None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "print len(plotdata)\n",
    "for i in plotdata:\n",
    "    if i[0] ==2877:\n",
    "#         print i[9]-1\n",
    "        print i[11]\n",
    "#         print fpeak[i[9]-1][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The de Ruiter distance as shown in the Trap Paper section 4.4 https://arxiv.org/abs/1503.01526 \n",
    "\n",
    "def CalcDeRuiter(transient,ra2,ra2e,dec2,dec2e):\n",
    "\n",
    "    ra1 = transient['ra']\n",
    "    dec1 = transient['dec']\n",
    "    ra1e = transient['rae']\n",
    "    dec1e = transient['dece']\n",
    "\n",
    "\n",
    "    r=(np.sqrt((((-1*ra2 +ra1)**2)*(np.cos((dec1+dec2)/2))**2)/\\\n",
    "                       (ra1e**2+ra2e**2)+((-1*dec2+dec1)**2/(dec1e**2+dec2e**2))))\n",
    "    return r "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the fluxes heavily inspired on the de Ruiter distance as used by Bart's thesis: https://api-alumni.nl/media/uploads/theses/phd/lha-scheers-phd.pdf page 55\n",
    "\n",
    "def CompareFluxes(transient,pflux2,pflux2e,scale=1):\n",
    "    pflux1 = transient['pflux']*scale\n",
    "    pflux1e = transient['pfluxe']*scale\n",
    "    \n",
    "    r = np.sqrt((((-1*pflux2 +pflux1)**2))/\\\n",
    "                       (pflux1e**2+pflux2e**2))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the frequency flux scale based on supplied califreq and califlux data\n",
    "\n",
    "def CalcFreqScale(transient,datafreq,califreq,califlux):\n",
    "    \"\"\"Function for calculating the frequency scale\"\"\"\n",
    "    transfreq = transient.freq\n",
    "    for i in range(len(califreq)):\n",
    "        if transfreq - 0.005 <= califreq[i] <= transfreq + 0.005:\n",
    "            oldflux = califlux[i]\n",
    "        if datafreq - 0.005 <= califreq[i] <= datafreq + 0.005:\n",
    "            newflux = califlux[i]\n",
    "    scale = newflux/oldflux\n",
    "    return scale\n",
    "\n",
    "def CalcRelFlux(transient,datafreq,dataflux):\n",
    "    transfreq = transient.freq\n",
    "    scale = np.power((transfreq/datafreq),-0.7)*dataflux \n",
    "    return scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left off here <a name='bookmark' />\n",
    "\n",
    "Go to <a href=#bookmark2>my bookmark</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for pruning our candidate list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for comparing transient candidate against external databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def CheckSimOutsideDatabase(data,sources,rcheck,ycheck,checklight,gamma,Cali,deruiter = True,euccutoff = 0.06):\n",
    "    \"\"\" Function for checking against outside database the data entry is for the database rcheck is condition \\\n",
    "where we check deruiter against, ycheck is where we check de flux against and checklight,gamma,deruiter\\\n",
    "are booleans which switch on checking the flux(checklight), checking using error on the flux(gamma) and \\\n",
    "using deruiter distance\n",
    "\n",
    "\"\"\"\n",
    "    masterindex = []\n",
    "    savedilist = []\n",
    "    print 'before:'\n",
    "    if Cali:\n",
    "        scale =  CalcFreqScale(sources[0],data.freq,califreq,califlux)\n",
    "    else:\n",
    "        scale = CalcRelFlux(sources[0],data.freq,data.pflux)\n",
    "\n",
    "    print len(sources)\n",
    "\n",
    "    for i in sources:\n",
    "        indexlist = []\n",
    "\n",
    "        if not deruiter:\n",
    "            Y = distance.cdist(i.radec,data.radec,'euclidean')\n",
    "            if gamma:\n",
    "                y = CompareFluxes(i,np.array(data.pflux),np.array(data.pfluxe),scale)\n",
    "                \n",
    "                for g in Y:\n",
    "\n",
    "                    for j in range(len(g)):\n",
    "\n",
    "                        if g[j] <=euccutoff and y[j]>= ycheck:\n",
    "                            if i.id not in savedilist:\n",
    "                                FluxVar(vars(i),j,data,scale)\n",
    "                                savedilist.append(i.id)\n",
    "                            indexlist.append(j)\n",
    "\n",
    "                        elif g[j] <=euccutoff:\n",
    "                            indexlist.append(j)\n",
    "                    \n",
    "            else:\n",
    "                for g in Y:\n",
    "\n",
    "                    for j in range(len(g)):\n",
    "                        if checklight:\n",
    "\n",
    "                            if g[j] <=euccutoff and (i.pflux*scale <= 0.9*data.pflux[j] or i.pflux*scale>=1.1*data.pflux[j]):\n",
    "                                if i.id not in savedilist:\n",
    "                                    FluxVar(vars(i),j,data,scale)\n",
    "                                    savedilist.append(i.id)\n",
    "                                indexlist.append(j)\n",
    "\n",
    "                            elif g[j] <=euccutoff:\n",
    "                                indexlist.append(j)\n",
    "                        else:\n",
    "                            if g[j] <=euccutoff:\n",
    "                                indexlist.append(j)\n",
    "\n",
    "        else:\n",
    "            r = CalcDeRuiter(vars(i),np.array(data.ra),np.array(data.rae),np.array(data.dec),np.array(data.dece))\n",
    "            if gamma:\n",
    "                y = CompareFluxes(vars(i),np.array(data.pflux),np.array(data.pfluxe),scale)\n",
    "\n",
    "                for j in range(len(r)):\n",
    "\n",
    "                    if r[j] <=rcheck:\n",
    "                        indexlist.append([j,r[j]])\n",
    "               \n",
    "                if checklight and indexlist:\n",
    "                    pfluxlist = []\n",
    "                    pfluxelist = []\n",
    "                    \n",
    "                    indexvalue = np.argmin(np.array(indexlist)[:,1])\n",
    "                    pfluxlist.append(data.pflux[indexlist[indexvalue][0]])\n",
    "                    pfluxelist.append(data.pfluxe[indexlist[indexvalue][0]])\n",
    "                    \n",
    "                    y = CompareFluxes(vars(i),np.array(pfluxlist),np.array(pfluxelist))\n",
    "\n",
    "                    \n",
    "                    if y[0]>=ycheck:\n",
    "\n",
    "                        FluxVar(vars(i),indexlist[indexvalue][0],data,scale)\n",
    "\n",
    "\n",
    "            else:\n",
    "                for j in range(len(r)):\n",
    "                    if checklight:\n",
    "                        if r[j] <=rcheck and (i.pflux*scale <= 0.9*data.pflux[j] or i.pflux*scale>=1.1*data.pflux[j]):\n",
    "                            indexlist.append(j)\n",
    "                            if i.id not in savedilist:\n",
    "                                FluxVar(vars(i),j,data,scale)\n",
    "                                savedilist.append(i.id)\n",
    "                        elif r[j] <=rcheck:\n",
    "                            indexlist.append(j)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        if r[j] <=rcheck:\n",
    "                            indexlist.append(j)\n",
    "\n",
    "        masterindex.append(indexlist)\n",
    "    \n",
    "#     check if thereis a zero listentry in masterindex.. if so append it to the varying pos sources class.\n",
    "    i = 0\n",
    "    while i < len(masterindex):\n",
    "        if not masterindex[i]:\n",
    "            PosVar(vars(sources[i]),data)\n",
    "        i+=1\n",
    "\n",
    "    print 'after:'  \n",
    "    print str(len(PosVar.instances)) +\" Interesting candidates\"\n",
    "    print str(len(FluxVar.instances)) + \" Flux Varying candidates\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class object for saving our candidate transients, class object for storing objects that have varying flux compared to external database and\n",
    "Initialize our transients class and other banana sources lists.\n",
    "\n",
    "(This piece of code has to be rerun everytime you adjust something)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Transients(object):\n",
    "    instances = []\n",
    "    def __init__(self,ra,rae,dec,dece,ids,pflux,pfluxe,url):\n",
    "        self.id = ids\n",
    "        y = np.stack((ra,dec),axis = -1)\n",
    "        self.url = url\n",
    "        self.ra = ra\n",
    "        self.rae = rae\n",
    "        self.dec = dec\n",
    "        self.dece = dece\n",
    "        self.pflux = pflux\n",
    "        self.pfluxe = pfluxe\n",
    "        self.freq = 144\n",
    "        self.keys = ['yoloswag']\n",
    "        self.radec = np.reshape(y,(1,2))\n",
    "        c = SkyCoord(self.ra, self.dec, frame='fk5', unit='deg')\n",
    "        self.fk5 = c.to_string('hmsdms',sep=' ',precision= 0)\n",
    "        self.c = c\n",
    "        Transients.instances.append(self)\n",
    "\n",
    "    def DelFalse(i):\n",
    "        del Transients.instances[i]\n",
    "\n",
    "class PosVar(object):\n",
    "    \"\"\"class object for storing objects that have a varying position compared to the database we are looking in\"\"\"\n",
    "    instances = []\n",
    "    def __init__(self,params,database):\n",
    "        self.id = params['id']            \n",
    "        self.ra = params['ra']\n",
    "        self.url = params['url']\n",
    "        self.database = database\n",
    "        self.dataname = database.name\n",
    "        self.dec = params['dec']\n",
    "        self.radec = params['radec']\n",
    "        self.pflux = params['pflux']\n",
    "        c = SkyCoord(params['ra'], params['dec'], frame='fk5', unit='deg')\n",
    "        self.fk5 = c.to_string('hmsdms',sep=' ',precision= 0)\n",
    "        self.c = c\n",
    "        try:\n",
    "            self.iflux = params['iflux']\n",
    "        except:\n",
    "            pass\n",
    "        PosVar.instances.append(self)\n",
    "        \n",
    "\n",
    "class TestInstance(object):\n",
    "    \"\"\"class object for storing objects that have a varying position compared to the database we are looking in\"\"\"\n",
    "    instances = []\n",
    "    def __init__(self,params):\n",
    "        self.id = params['id']            \n",
    "        self.ra = params['ra']\n",
    "        self.url = params['url']\n",
    "        self.database = params['database']\n",
    "        self.dataname = params['dataname']\n",
    "        self.dec = params['dec']\n",
    "        self.radec = params['radec']\n",
    "        self.pflux = params['pflux']\n",
    "        c = SkyCoord(params['ra'], params['dec'], frame='fk5', unit='deg')\n",
    "        self.fk5 = c.to_string('hmsdms',sep=' ',precision= 0)\n",
    "        self.c = c\n",
    "        try:\n",
    "            self.iflux = params['iflux']\n",
    "        except:\n",
    "            pass\n",
    "        TestInstance.instances.append(self)\n",
    "        \n",
    "class SavedFluxSources(object):\n",
    "    \"\"\"class object for storing objects that have a varying position compared to the database we are looking in\"\"\"\n",
    "    instances = []\n",
    "    def __init__(self,initialid,initialpflux,initialpfluxerr,params):\n",
    "        self.id = params['id']            \n",
    "        self.ra = params['ra']\n",
    "        self.url = params['url']\n",
    "        self.dec = params['dec']\n",
    "        self.rae = params['rae']\n",
    "        self.dece = params['dece']\n",
    "        self.radec = params['radec']\n",
    "        self.pflux = params['pflux']\n",
    "        self.freq = params['freq']\n",
    "        self.pfluxe = params['pfluxe']\n",
    "        c = SkyCoord(params['ra'], params['dec'], frame='fk5', unit='deg')\n",
    "        self.fk5 = c.to_string('hmsdms',sep=' ',precision= 0)\n",
    "        self.c = c\n",
    "        self.cmpid = initialid\n",
    "        self.cmppflux = initialpflux\n",
    "        self.cmppfluxerr = initialpfluxerr\n",
    "        try:\n",
    "            self.iflux = params['iflux']\n",
    "        except:\n",
    "            pass\n",
    "        SavedFluxSources.instances.append(self)\n",
    "\n",
    "class SavedPosSources(object):\n",
    "    \"\"\"class object for storing objects that have a varying position compared to the database we are looking in\"\"\"\n",
    "    instances = []\n",
    "    def __init__(self,params):\n",
    "        self.id = params['id']            \n",
    "        self.ra = params['ra']\n",
    "        self.url = params['url']\n",
    "        self.dec = params['dec']\n",
    "        self.rae = params['rae']\n",
    "        self.dece = params['dece']\n",
    "        self.radec = params['radec']\n",
    "        self.pflux = params['pflux']\n",
    "        self.freq = params['freq']\n",
    "        self.pfluxe = params['pfluxe']\n",
    "        c = SkyCoord(params['ra'], params['dec'], frame='fk5', unit='deg')\n",
    "        self.fk5 = c.to_string('hmsdms',sep=' ',precision= 0)\n",
    "        self.c = c\n",
    "        try:\n",
    "            self.iflux = params['iflux']\n",
    "        except:\n",
    "            pass\n",
    "        SavedPosSources.instances.append(self)  \n",
    "    \n",
    "class FluxVar(object):\n",
    "    \"\"\"class object for storing objects that have a varying flux compared to the database we are looking in\"\"\"\n",
    "    instances = []\n",
    "    def __init__(self,params,databaseentry,database,scale):\n",
    "        self.id = params['id']\n",
    "        self.ra = params['ra']\n",
    "        self.url = params['url']\n",
    "        self.database = database\n",
    "        self.dataname = database.name\n",
    "        self.scale = scale\n",
    "        self.dec = params['dec']\n",
    "        self.radec = params['radec']\n",
    "        self.pflux = params['pflux']\n",
    "        self.entry = databaseentry\n",
    "        c = SkyCoord(params['ra'], params['dec'], frame='fk5', unit='deg')\n",
    "        self.fk5 = c.to_string('hmsdms',sep=' ',precision= 0)\n",
    "        self.c = c\n",
    "        c = c.icrs\n",
    "        self.icrs = c.to_string('hmsdms',sep=' ',precision= 0)\n",
    "        try:\n",
    "            self.iflux = params['iflux']\n",
    "        except:\n",
    "            pass\n",
    "        FluxVar.instances.append(self)\n",
    "    \n",
    "\n",
    "        \n",
    "racata = []\n",
    "raecata = []\n",
    "deccata = []\n",
    "dececata = []\n",
    "idcata = []\n",
    "lightcata = []\n",
    "lightecata = []\n",
    "idstorelist = []\n",
    "for i in range(len(plotdata)):\n",
    "    if plotdata[i][0] in idstorelist:\n",
    "        continue\n",
    "    else:\n",
    "        if plotdata[i][6] == 1:\n",
    "            Transients(plotdata[i][1],plotdata[i][3],plotdata[i][2],plotdata[i][4],plotdata[i][0],fpeak[plotdata[i][9]-1][0],fpeake[plotdata[i][9]-1][0],[plotdata[i][11]])\n",
    "        else:\n",
    "            racata.append(plotdata[i][1])\n",
    "            raecata.append(plotdata[i][3])\n",
    "            deccata.append(plotdata[i][2])\n",
    "            dececata.append(plotdata[i][4])\n",
    "            idcata.append(plotdata[i][0])\n",
    "#           -1 necessary to account for pythonic lists\n",
    "            lightcata.append(fpeak[plotdata[i][9]-1][0])\n",
    "            lightecata.append(fpeake[plotdata[i][9]-1][0])\n",
    "        idstorelist.append(plotdata[i][0])\n",
    "X = np.stack((racata,deccata),axis = -1)\n",
    "\n",
    "racata = np.array(racata)\n",
    "raecata = np.array(raecata)\n",
    "dececata = np.array(dececata)\n",
    "deccata = np.array(deccata)\n",
    "idcata = np.array(idcata)\n",
    "lightcata = np.array(lightcata)\n",
    "lightecata = np.array(lightecata)\n",
    "\n",
    "\n",
    "# Transients(183,0.01,45,0.01,99999999,1.32,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rasl = []\n",
    "# raesl = []\n",
    "# decsl = []\n",
    "# decesl = []\n",
    "# idsl = []\n",
    "# pfluxsl = []\n",
    "# pfluxesl = []\n",
    "# for i in range(len(plotdata)):\n",
    "#     rasl.append(plotdata[i][1])\n",
    "#     raesl.append(plotdata[i][3])\n",
    "#     decsl.append(plotdata[i][2])\n",
    "#     decesl.append(plotdata[i][4])\n",
    "#     idsl.append(plotdata[i][0])\n",
    "#     pfluxsl.append(fpeak[plotdata[i][9]-1][0])\n",
    "#     pfluxesl.append(fpeake[plotdata[i][9]-1][0])\n",
    "\n",
    "\n",
    "\n",
    "# raseries = pd.Series(racata)\n",
    "# raseries\n",
    "# ras = pd.Series(rasl, name='ra')\n",
    "# raes = pd.Series(raesl, name='raerr')\n",
    "# decs = pd.Series(decsl, name='dec')\n",
    "# deces = pd.Series(decesl, name='decerr')\n",
    "# ids = pd.Series(idsl, name='id')\n",
    "# pfluxs = pd.Series(pfluxsl, name='pflux')\n",
    "# pfluxes = pd.Series(pfluxesl, name='pfluxerr')\n",
    "\n",
    "# pandacata = pd.concat([ras, raes,decs,deces,ids,pfluxs,pfluxes], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we call the function for calculating the de ruiter distance between candidates and our found sources and if True is given we also check if the max of the lightcurve of the sources compared fall withing 5 sigma flux of each other (thus making it increasingly likely for   the source to be a sidelobe)\n",
    "\n",
    "This extra flux comparison is used in order to not potentially write of a faint or bright transient close to a known source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SelectorForCandidateslist(rcheck,ycheck,checklight,deruiter=True):\n",
    "    \"\"\"Function for pruning the candidate list using both distance and lightcurve measurements\"\"\"\n",
    "    ctr = 0\n",
    "    print 'before:'\n",
    "    print len(Transients.instances)\n",
    "    oldlen = len(Transients.instances) + 1\n",
    "    \n",
    "# #   Repeat until no improvement in the length of the candidate list\n",
    "#     while len(Transients.instances) - oldlen < 0:\n",
    "    ratlist = []\n",
    "    detclist = []\n",
    "\n",
    "    translist = []\n",
    "#         rlist = []\n",
    "    oldlen = len(Transients.instances)\n",
    "\n",
    "#         These lists are necessary to compare the candidate Transientss against each other\n",
    "    ralist = []\n",
    "    raelist = []\n",
    "    declist = []\n",
    "    decelist = []\n",
    "\n",
    "# \"\"\"Check candidate Transientss against each other\"\"\"\n",
    "\n",
    "    for h in Transients.instances:\n",
    "        if not deruiter:\n",
    "            translist.append(h.radec)\n",
    "        else:\n",
    "            ralist.append(h.ra)\n",
    "            raelist.append(h.rae)\n",
    "            declist.append(h.dec)\n",
    "            decelist.append(h.dece)\n",
    "            \n",
    "\n",
    "    masterindex = []\n",
    "    savedlist = []\n",
    "    first = True\n",
    "    for i in Transients.instances:\n",
    "        nocount = False\n",
    "        indexlist = []\n",
    "\n",
    "\n",
    "# #             again the outdated euclidean method\n",
    "#         if not deruiter:\n",
    "#             Y = distance.cdist(i.radec,X,'euclidean')\n",
    "\n",
    "#             for g in Y:\n",
    "#                 for j in range(len(g)):\n",
    "#                     if checklight:\n",
    "#                         if g[j] <=0.06 and 0.8*lightcata[j]<=i.pflux <= 1.2*lightcata[j]:\n",
    "#                             indexlist.append(j)\n",
    "#                     else:\n",
    "#                         if g[j] <=0.06:\n",
    "#                             indexlist.append(j)\n",
    "\n",
    "#             the de Ruiter method \n",
    "#         else:\n",
    "        r = CalcDeRuiter(vars(i),np.array(ralist),\\\n",
    "        np.array(raelist),np.array(declist),np.array(decelist))\n",
    "        y = CompareFluxes(vars(i),np.array(lightcata),np.array(lightecata))\n",
    "        for j in range(len(r)):\n",
    "\n",
    "            if r[j] <=rcheck and r[j]!=0.0:\n",
    "                indexlist.append(j)\n",
    "                nocount = True\n",
    "                        \n",
    "#         SAVE ENTRIES THAT HAVE A BIG DIFFERENCE ON FLUX\n",
    "        if checklight:\n",
    "            pfluxlist = []\n",
    "            pfluxelist = []\n",
    "            for z in indexlist:            \n",
    "                pfluxlist.append(Transients.instances[z].pflux)\n",
    "                pfluxelist.append(Transients.instances[z].pfluxe)\n",
    "\n",
    "            y = CompareFluxes(vars(i),np.array(pfluxlist),np.array(pfluxelist))\n",
    "            \n",
    "            if first:\n",
    "                print y\n",
    "                first = False\n",
    "                try:\n",
    "                    print Transients.instances[indexlist[0]].id\n",
    "                except:\n",
    "                    pass\n",
    "            for o in range(len(y)):\n",
    "                if y[o]>=ycheck:\n",
    "                    SavedFluxSources(i.id,i.pflux,i.pfluxe,vars(Transients.instances[indexlist[o]]))\n",
    "                    break\n",
    "                    \n",
    "                 \n",
    "#         DELETE DOUBLE ENTRIES reform lists\n",
    "        if nocount:\n",
    "            k = 0\n",
    "            g = 0\n",
    "            while k < len(indexlist):\n",
    "                del Transients.instances[indexlist[k]-g]\n",
    "\n",
    "                k+=1\n",
    "                g+=1\n",
    "\n",
    "            ralist = []\n",
    "            raelist = []\n",
    "            declist = []\n",
    "            decelist = []\n",
    "\n",
    "            for h in Transients.instances:\n",
    "                ralist.append(h.ra)\n",
    "                raelist.append(h.rae)\n",
    "                declist.append(h.dec)\n",
    "                decelist.append(h.dece)\n",
    "\n",
    "\n",
    "    print 'after:'\n",
    "#     add back the no duplicate detections\n",
    "#     for i in SavedPosSources.instances:\n",
    "#         TestInstance(vars(i))\n",
    "    print len(Transients.instances)\n",
    "    print len(SavedFluxSources.instances)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:\n",
      "34\n",
      "[1.4800436]\n",
      "67\n",
      "after:\n",
      "18\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "SelectorForCandidateslist(rcheck = 3, ycheck = 3,checklight = True, deruiter = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print out in latex format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0836510414567\n",
      "0.0833667476981\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print SavedFluxSources.instances[0].pflux\n",
    "    print Transients.instances[0].pflux\n",
    "except:\n",
    "    pass\n",
    "FluxVar.instances = []\n",
    "PosVar.instances = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to <a href=#bookmark>my bookmark</a>\n",
    "Left off here <a name='bookmark2' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tgss \n",
    "is the object which has tggs survey data for the field of P23 loaded into it.\n",
    "could probably just automate it here but lets leave it for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name', 'radec', 'ra', 'rae', 'dec', 'dece', 'pflux', 'pfluxe', 'freq']\n",
      "147.5\n",
      "tgss\n",
      "98604\n",
      "0.0006028485434442595\n"
     ]
    }
   ],
   "source": [
    "tgsslist = dl.ReadData(\"tgss\")\n",
    "tgss = tgsslist[1]\n",
    "print tgss.keys\n",
    "print tgss.freq\n",
    "print tgss.name\n",
    "print len(tgss.ra)\n",
    "print np.mean(tgss.rae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:\n",
      "18\n",
      "after:\n",
      "7 Interesting candidates\n",
      "1 Flux Varying candidates\n"
     ]
    }
   ],
   "source": [
    "CheckSimOutsideDatabase(tgss,Transients.instances,rcheck = 3,ycheck = 3,\\\n",
    "                     checklight = True, gamma = True,Cali = True,deruiter = True,euccutoff = .06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 20 04 +47 21 47\n",
      "<SkyCoord (FK5: equinox=J2000.000): (ra, dec) in deg\n",
      "    (185.01864575, 47.36309713)>\n"
     ]
    }
   ],
   "source": [
    "print Transients.instances[0].fk5\n",
    "print Transients.instances[0].c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VLSSr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vlssrimage.fits\n",
      "vlssrdataset.xls\n",
      "*** No CODEPAGE record, no encoding_override: will use 'ascii'\n",
      "*** No CODEPAGE record, no encoding_override: will use 'ascii'\n",
      "*** No CODEPAGE record, no encoding_override: will use 'ascii'\n",
      "*** No CODEPAGE record, no encoding_override: will use 'ascii'\n",
      "['name', 'radec', 'ra', 'rae', 'dec', 'dece', 'pflux', 'pfluxe', 'freq']\n",
      "74\n",
      "[185.2953 184.7767 185.0477 ... 214.8349 165.3226 213.6304]\n"
     ]
    }
   ],
   "source": [
    "# TODO fix the error for the vlssr data\n",
    " \n",
    "# Probably need to pip install xlrd for excell support in pandas\n",
    "vlssr = dl.ReadData('vlssr')[0]\n",
    "\n",
    "print vlssr.keys\n",
    "print vlssr.freq\n",
    "print vlssr.ra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:\n",
      "18\n",
      "after:\n",
      "25 Interesting candidates\n",
      "1 Flux Varying candidates\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CheckSimOutsideDatabase(vlssr,Transients.instances,rcheck = 3 ,ycheck = 3,checklight = True,\\\n",
    "                     gamma = True,Cali=True,deruiter = True,euccutoff = 0.06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "1\n",
      "18\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "savedpos = []\n",
    "savedflux = []\n",
    "print len(PosVar.instances)\n",
    "print len(FluxVar.instances)\n",
    "for i in PosVar.instances:\n",
    "    if i.id not in savedpos:\n",
    "        savedpos.append(i.id)\n",
    "for i in FluxVar.instances:\n",
    "    if i.id not in savedflux:\n",
    "        savedflux.append(i.id)\n",
    "        \n",
    "print len(savedpos)\n",
    "print len(savedflux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print saved flux sources LATEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print \"\\\\begin{longtable}{c|c|c|c|c|c|c|c|c|c} \\n\\\n",
    "# \\\\toprule \\n Id & Ra (deg) & Dec (deg) & Fk5 & Flux & Fluxerr & cmpId & cmpFlux & cmpFluxerr & Candidate? (y/n)  \\\\\\\\\\\\midrule \\n\\\\endfirsthead \\n\\\\toprule \\\n",
    "# \\n Id & Ra (deg) & Dec (deg) & Fk5 & Flux & Fluxerr & cmpId & cmpFlux & cmpFluxerr & Candidate? (y/n)  \\\\\\\\\\\\midrule \\n\\\\endhead \\n\\\\bottomrule \\n\\\\endfoot \\n\\\\bottomrule\"\n",
    "\n",
    "# # define caption for table\n",
    "# print \"\\\\caption{ SAVED FLUX SOURCES} \\n\\\\endlastfoot\"\n",
    "# for i in SavedFluxSources.instances:\n",
    "#     print latexHREF + str(i.id) +\"}{%i}\" %(i.id)+ \" \" + \"&\"+ \" \" + '%0.3f' %i.ra + \" \" + \"&\"+ \" \"+ \"%0.3f\" %(i.dec) +\\\n",
    "#     \" \" + \"&\" + i.fk5+\"&\" + \" \" + '%0.3f' %(i.pflux) +\"&\" + \" \" + '%0.3f' %(i.pfluxe) +\"&\" + \" \" +\\\n",
    "#     latexHREF + str(i.cmpid) +\"}{%i}\" %(i.cmpid)\\\n",
    "#     +\"&\" + \" \" +'%0.3f' %(i.cmppflux) +\"&\" + \" \"\\\n",
    "#     + '%0.3f' %(i.cmppfluxerr) +\"&\" + \" \" +\"\\\\\" + \"\\\\\"\n",
    "# print \"\\\\end{longtable}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the same position Fluxvariating sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data compared to tgss\n",
      "91\n",
      "http://banana.transientskp.org/r4/vlo_KmeulenSimSource/runningcatalog/91\n",
      "bananaflux: 0.044611882417634086\n",
      "tgssflux: 0.117\n",
      "185.39885\n",
      "47.54699\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in FluxVar.instances:\n",
    "    \n",
    "    print \"Data compared to \" + i.database.name\n",
    "    print i.id\n",
    "    print websiteURL+str(i.id)\n",
    "    print 'bananaflux: ' + str(i.pflux*i.scale)\n",
    "\n",
    "    print i.database.name+'flux: ' + str(i.database.pflux[i.entry])\n",
    "    print i.database.ra[i.entry]\n",
    "    print i.database.dec[i.entry]\n",
    "    print \"------------------------\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the same position Fluxvariating sources in Latex ready format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print \"\\\\begin{landscape}\\n\"\n",
    "# print \"\\\\section{Fluxvariating Sources}\\n\"\n",
    "# # SEPERATE DATABASES\n",
    "# databaselist = ['tgss','vlssr']\n",
    "\n",
    "# samesieslist = []\n",
    "# for i in range(len(FluxVar.instances)):\n",
    "#     samesie = False\n",
    "#     tempdict = {'tgss':None,'vlssr':None,'id':None,'ra':None,'dec':None,'fk5':None}\n",
    "    \n",
    "# #   create combined name if entry both in vlssr and tgss\n",
    "#     for j in range(len(samesieslist)):\n",
    "#         if FluxVar.instances[i].id == samesieslist[j]['id']:\n",
    "# #             samesieslist[j].append(FluxVar.instances[i])\n",
    "#             samesieslist[j][FluxVar.instances[i].database.name] = FluxVar.instances[i]\n",
    "          \n",
    "#             samesie = True\n",
    "#     if not samesie:\n",
    "#         tempdict[FluxVar.instances[i].database.name] = FluxVar.instances[i]\n",
    "#         tempdict['id'] = FluxVar.instances[i].id\n",
    "#         tempdict['ra'] = FluxVar.instances[i].ra\n",
    "#         tempdict['dec'] = FluxVar.instances[i].dec\n",
    "#         tempdict['fk5'] = FluxVar.instances[i].fk5\n",
    "#         samesieslist.append(tempdict)\n",
    "\n",
    "\n",
    "# print \"\\\\begin{longtable}{c|c|c|c|c|c|c|c|c|c|c|c|c} \\n\\\n",
    "# \\\\toprule \\nFlux147.5MHz (Jy) & Tgss (Jy) & Flux74MHz (Jy)& Vlssr (Jy) & fk5 & Ra (deg) & Dec (deg) &tggsRa&tgssDec&\\\n",
    "# vlssrRa&vlssrDec& Id/Link & (y/n)  \\\\\\\\\\\\midrule \\n\\\\endfirsthead \\n\\\\toprule \\\n",
    "# \\nFlux147.5MHz (Jy) & Tgss (Jy) & Flux74MHz (Jy)& Vlssr (Jy)& fk5 & Ra (deg) & Dec (deg)  &tggsRa&tgssDec&\\\n",
    "# vlssrRa&vlssrDec& Id/Link & (y/n) \\\\\\\\\\\\midrule \\n\\\\endhead \\n\\\\bottomrule \\n\\\\endfoot \\n\\\\bottomrule\"\n",
    "\n",
    "# # define caption for table\n",
    "# print \"\\\\caption{ Comparison of variable fluxes with the same position for different databases 1hr} \\n\\\\endlastfoot\"\n",
    "# lastname = None\n",
    "# # samesieslist = sorted(samesieslist, key=lambda e: (e['ra'], e['dec']))\n",
    "# for i in samesieslist:\n",
    "#     a= ''\n",
    "#     b=''\n",
    "#     for j in(databaselist):\n",
    "#         try: \n",
    "#             a+= str(np.around(i[j].pflux*i[j].scale,3)) + \" \" + \"&\"+ \" \"\n",
    "#         except:\n",
    "#             a += \"-\" +\" \"+ \"&\"+ \" \"\n",
    "#         try:\n",
    "#             a+= str(np.around(i[j].database.pflux[i[j].entry],3)) + \" \" + \"&\"+ \" \"   \n",
    "#         except:\n",
    "#             a += \"-\" +\" \"+ \"&\"+ \" \"\n",
    "            \n",
    "#         try: \n",
    "#             b+= '%0.3f' %(i[j].database.ra[i[j].entry])+\" \"+\"&\"+\" \"\n",
    "#             b+= '%0.3f' %(i[j].database.dec[i[j].entry])+\" \"+\"&\"+\" \"\n",
    "#         except:\n",
    "#             b+= \"-\" + \" \" + \"&\"+\" \" \"-\" + \" \" + \"&\"+\" \"\n",
    "        \n",
    "#     a +=i['fk5'] + \" \" + '&' + \"  \"+ '%0.3f' %i['ra'] + \" \" + \"&\"+ \" \"+ \"%0.3f\" %i['dec'] \\\n",
    "#     + \" \" + '&' + \"  \"+ b+ latexHREF +str(i[\"id\"])+\"}\" + \"{%i}\" %i['id'] +\" \"+\"&\"+\" \"+  \"\\\\\" + \"\\\\\"\n",
    "#     print a\n",
    "\n",
    "# print \"\\\\end{longtable}\\n\"\n",
    "# print \"\\\\end{landscape}\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the Position variating sources in Latex ready format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SEPERATE DATABASES\n",
    "\n",
    "# print \"\\\\begin{longtable}{c|c|c|c|c|c} \\n\\\n",
    "# \\\\toprule \\nDatabase & Ra (deg) & Dec (deg) & Fk5 & Id/Link & Candidate? (y/n)  \\\\\\\\\\\\midrule \\n\\\\endfirsthead \\n\\\\toprule \\\n",
    "# \\nDatabase & Ra (deg) & Dec (deg) & Fk5 & Id/Link & Candidate? (y/n)  \\\\\\\\\\\\midrule \\n\\\\endhead \\n\\\\bottomrule \\n\\\\endfoot \\n\\\\bottomrule\"\n",
    "\n",
    "# # define caption for table\n",
    "# print \"\\\\caption{ Comparison position of candidates with different databases for the 10 min interval data} \\n\\\\endlastfoot\"\n",
    "# lastname = None\n",
    "# for i in PosVar.instances:\n",
    "#     if i.database.name != lastname and lastname:\n",
    "#         print \"\\\\hline\"\n",
    "#     print i.database.name +\" \"+ \"&\"+ \" \" + '%0.3f' %i.ra + \" \" + \"&\"+ \" \"+ \"%0.3f\" %i.dec +\\\n",
    "#     \" \" + \"&\" + \" \" + i.fk5 +\" \" + '&' + \"  \"+ latexHREF +str(i.id)+\"}\" + \"{%i}\" %i.id +\" \" + '&' + \"  \"+\"\\\\\" + \"\\\\\"\n",
    "    \n",
    "#     lastname = i.database.name\n",
    "\n",
    "# print \"\\\\end{longtable}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # COMBINED DATABASES (SLOWER)\n",
    "# print \"\\\\begin{1andscape}\\n\"\n",
    "# print \"\\\\section{Position variating Sources}\\n\"\n",
    "# samesieslist = []\n",
    "# for i in range(len(PosVar.instances)):\n",
    "#     samesie = False\n",
    "    \n",
    "# #   create combined name if entry both in vlssr and tgss\n",
    "#     for j in range(len(samesieslist)):\n",
    "#         if PosVar.instances[i].id == samesieslist[j][0]:\n",
    "#             samesieslist[j][1] = samesieslist[j][1] + \" \\& \" + PosVar.instances[i].database.name\n",
    "#             samesie = True\n",
    "#     if not samesie:\n",
    "#         samesieslist.append([PosVar.instances[i].id,PosVar.instances[i].database.name,PosVar.instances[i]])\n",
    "    \n",
    "# samesieslist = sorted(samesieslist, key=itemgetter(1))\n",
    "\n",
    "# print \"\\\\begin{longtable}{c|c|c|c|c|c} \\n\\\n",
    "# \\\\toprule \\nDatabase & Ra (deg) & Dec (deg)& Fk5 & Id/Link & Candidate? (y/n)  \\\\\\\\\\\\midrule \\n\\\\endfirsthead \\n\\\\toprule \\\n",
    "# \\nDatabase & Ra (deg) & Dec (deg) & Fk5 &Id/Link & Candidate? (y/n)  \\\\\\\\\\\\midrule \\n\\\\endhead \\n\\\\bottomrule \\n\\\\endfoot \\n\\\\bottomrule\"\n",
    "\n",
    "# # define caption for table\n",
    "# print \"\\\\caption{ Comparison position of candidates with different databases for the 1hr interval data} \\n\\\\endlastfoot\"\n",
    "# lastname = None\n",
    "# for i in samesieslist:\n",
    "#     if i[1] != lastname and lastname:\n",
    "#         print \"\\\\hline\"\n",
    "#     print i[1] +\" \"+ \"&\"+ \" \"+ '%0.3f' %i[2].ra + \" \" + \"&\"+ \" \"+ \"%0.3f\" %(i[2].dec) +\\\n",
    "#     \" \" + \"&\" + \" \" +i[2].fk5 +\" \" + '&' + \"  \"+ latexHREF +str(i[0])+\"}\" + \"{%i}\" %i[0] +\" \" + '&' + \"  \"+\"\\\\\" + \"\\\\\"\n",
    "    \n",
    "#     lastname = i[1]\n",
    "\n",
    "# print \"\\\\end{longtable}\\n\"\n",
    "# print \"\\\\end{landscape}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonwriter.WriteFluxJson(FluxVar.instances)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allready removed\n",
      "/home/kmeulen/virtualenv/local/lib/python2.7/site-packages/cryptography/hazmat/primitives/constant_time.py:26: CryptographyDeprecationWarning: Support for your Python version is deprecated. The next version of cryptography will remove support. Please upgrade to a 2.7.x release that supports hmac.compare_digest as soon as possible.\n",
      "  utils.DeprecatedIn23,\n",
      "2018-10-16 19:11:29 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: scrapybot)\n",
      "2018-10-16 19:11:29 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 2.7.6 (default, Nov 23 2017, 15:49:48) - [GCC 4.8.4], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Linux-3.13.0-141-generic-x86_64-with-Ubuntu-14.04-trusty\n",
      "2018-10-16 19:11:29 [scrapy.crawler] INFO: Overridden settings: {'SPIDER_LOADER_WARN_ONLY': True, 'FEED_FORMAT': 'json', 'FEED_URI': 'neat.json'}\n",
      "2018-10-16 19:11:29 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "2018-10-16 19:11:29 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2018-10-16 19:11:29 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2018-10-16 19:11:29 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2018-10-16 19:11:29 [scrapy.core.engine] INFO: Spider opened\n",
      "2018-10-16 19:11:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2018-10-16 19:11:29 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
      "2018-10-16 19:11:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://vizier.u-strasbg.fr/viz-bin/VizieR-3?-source=VIII/85A/spectra&-c=185.3986%2B47.5469&-c.u=arcsec&-c.r=5&-out.add=_r&-sort=_r> (referer: None)\n",
      "2018-10-16 19:11:30 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://vizier.u-strasbg.fr/viz-bin/VizieR-4> (referer: http://vizier.u-strasbg.fr/viz-bin/VizieR-3?-source=VIII/85A/spectra&-c=185.3986%2B47.5469&-c.u=arcsec&-c.r=5&-out.add=_r&-sort=_r)\n",
      "2018-10-16 19:11:30 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2018-10-16 19:11:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 1963,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/request_method_count/POST': 1,\n",
      " 'downloader/response_bytes': 70040,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2018, 10, 16, 17, 11, 30, 983748),\n",
      " 'log_count/DEBUG': 3,\n",
      " 'log_count/INFO': 7,\n",
      " 'memusage/max': 45649920,\n",
      " 'memusage/startup': 45649920,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2018, 10, 16, 17, 11, 29, 97057)}\n",
      "2018-10-16 19:11:30 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No JSON object could be decoded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-ff392ad0bf36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdfFlux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mljs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/kmeulen/notebooks/KrieksNotebook/KrieksTransients/loadjson.pyc\u001b[0m in \u001b[0;36mLoadin\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mLoadin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mjsonFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"neat.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjsonFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/json/__init__.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0mparse_constant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_constant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_pairs_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_pairs_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         **kw)\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/json/__init__.pyc\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/json/decoder.pyc\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \"\"\"\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/json/decoder.pyc\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No JSON object could be decoded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No JSON object could be decoded"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.remove('neat.json')\n",
    "except:\n",
    "    print 'allready removed'\n",
    "\n",
    "!{sys.executable} -m scrapy runspider scrapper.py -o neat.json\n",
    "    \n",
    " \n",
    "dfFlux = ljs.Loadin()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# models to try\n",
    "def starmodel(value,a,b,c):\n",
    "    return a*value**-b+c\n",
    "\n",
    "def starmodel2(value,*par):\n",
    "    return 10**(-np.log(value)*par[0] + par[1])\n",
    "\n",
    "def starmodel3(value,*par):\n",
    "    return 10**(-np.log(value*par[0])-np.exp(value)*par[1] + par[2])\n",
    "\n",
    "def ChiSq2(parm, xval, yval, dy): # the weighted least-squares\n",
    "    ymod = starmodel2(xval,*parm)\n",
    "    chisq = sum(pow((yval-ymod)/dy,2))\n",
    "    return chisq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matplotlibfunction(frame,freq,flux,fluxerr,colour):\n",
    "    \n",
    "    \n",
    "    #xstart, ystart, xend, yend [units are fraction of the image frame, from bottom left corner]\n",
    "    \n",
    "    plt.errorbar(freq,flux,yerr = fluxerr,color=colour,fmt = '.',ecolor='black') #Noisy data\n",
    "\n",
    "\n",
    "# get our own data\n",
    "def getSourceData2(frame,runcat_id,colour):\n",
    "    runcat = session.query(tkp.db.model.Runningcatalog).filter(tkp.db.model.Runningcatalog.id==runcat_id).one()\n",
    "    sources = runcat.extractedsources\n",
    "    \n",
    "    flux = []\n",
    "    fluxerr = []\n",
    "    freq = []\n",
    "    \n",
    "    for s in sources:\n",
    "        if s.extract_type == 0:\n",
    "            flux.append(s.f_int*10**3)\n",
    "            fluxerr.append(s.f_int_err*10**3)\n",
    "            freq.append(s.image.freq_eff/(1.*10**6))\n",
    "\n",
    "    matplotlibfunction(frame,freq,flux,fluxerr,colour)\n",
    "# #Residual plot\n",
    "# difference = Fofx(x,*p) - ydata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot function\n",
    "def bokehplotfunction(d,freq,flux,fluxerr,colour):\n",
    "    lower = []\n",
    "    upper = []\n",
    "    base =[]\n",
    "    for i in range(len(flux)):\n",
    "        lower.append(flux[i]-fluxerr[i])\n",
    "        upper.append(flux[i]+fluxerr[i])\n",
    "        base.append(freq[i])\n",
    "\n",
    "    d.scatter(freq,flux,color =colour)\n",
    "    source_error = ColumnDataSource(data=dict(base=base, lower=lower, upper=upper))\n",
    "\n",
    "    d.add_layout(\n",
    "        Whisker(source=source_error, base=\"base\", upper=\"upper\", lower=\"lower\")\n",
    "    )\n",
    "    return d\n",
    "\n",
    "# make a fit\n",
    "def fitexistingdata(freq,flux,fluxerr,ids,x):\n",
    "    a=0.001\n",
    "    b = 10\n",
    "    c = 100\n",
    "\n",
    "        \n",
    "    p01=[b,c]\n",
    "    fitfunction = starmodel2\n",
    "        \n",
    "    ml_cfpars, ml_cfcovar = op.curve_fit(fitfunction, freq, flux, p01, sigma=fluxerr)\n",
    "    err = np.sqrt(np.diag(ml_cfcovar))\n",
    "    confidence = np.diag(ml_cfcovar)\n",
    "    fit = fitfunction(x, *ml_cfpars)\n",
    "    dof = len(freq)-len(p01)\n",
    "    chisquare = ChiSq2(ml_cfpars,freq,flux,fluxerr)\n",
    "    return fit,chisquare,dof,ml_cfpars, ml_cfcovar\n",
    "\n",
    "# get our own data\n",
    "def getSourceData(d,runcat_id,colour):\n",
    "    runcat = session.query(tkp.db.model.Runningcatalog).filter(tkp.db.model.Runningcatalog.id==runcat_id).one()\n",
    "    sources = runcat.extractedsources\n",
    "    \n",
    "    flux = []\n",
    "    fluxerr = []\n",
    "    freq = []\n",
    "    \n",
    "    for s in sources:\n",
    "        if s.extract_type == 0:\n",
    "            flux.append(s.f_int*10**3)\n",
    "            fluxerr.append(s.f_int_err*10**3)\n",
    "            freq.append(s.image.freq_eff/(1.*10**6))\n",
    "\n",
    "    bokehplotfunction(d,freq,flux,fluxerr,colour)\n",
    "    return d,freq,flux,fluxerr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN THE FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "storedfitvalueslist = []\n",
    "# initialize x\n",
    "x = np.arange(1e1, 1e4, 1)\n",
    "\n",
    "# loops through the data gathered sorting by id and plotting the fit\n",
    "\n",
    "for i in dfFlux.id.unique():\n",
    "    specid = dfFlux[dfFlux.id == i]\n",
    "    ids = int(i)\n",
    "    freq = np.array(specid.nu.astype(float))\n",
    "    flux = np.array(specid.s_nu.astype(float))\n",
    "    fluxerr = np.array(specid.e.astype(float))\n",
    "    \n",
    "    \n",
    "#     Append catalogue data to the set:\n",
    "    for j in FluxVar.instances:\n",
    "        if j.id == ids:\n",
    "            freq = np.append(freq,j.database.freq)\n",
    "            flux = np.append(flux,j.database.pflux[j.entry]*10**3 )\n",
    "            fluxerr = np.append(fluxerr,j.database.pfluxe[j.entry]*10**3 )\n",
    "\n",
    "    print freq\n",
    "    \n",
    "    #Initialize figure\n",
    "    fig1 = plt.figure(1,figsize=(10,10))\n",
    "    fig1.suptitle(\"Plot of: \" + str(ids),fontsize=20)\n",
    "    \n",
    "    #add first frame\n",
    "    frame1=fig1.add_axes((.1,.3,.8,.6))\n",
    "    matplotlibfunction(frame1,freq,flux,fluxerr,\"blue\")\n",
    "    \n",
    "\n",
    "    \n",
    "    try:\n",
    "        fit,chisquare,dof,ml_cfpars, ml_cfcovar = fitexistingdata(freq,flux,fluxerr,ids,x)\n",
    "\n",
    "    except:\n",
    "        print \"for id: %i there is no good fit\" %ids\n",
    "        print freq\n",
    "\n",
    "    \n",
    "    try:\n",
    "        print \"with a goodness of fit of: %0.3f and a dof of: %0.1f\" %(chisquare,dof)\n",
    "        FluxVar\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    getSourceData2(frame1,int(ids),colour = 'red')\n",
    "    \n",
    "#   initialize normal frame\n",
    "    frame1.set_xscale('log')\n",
    "    frame1.set_yscale('log')\n",
    "    frame1.set_xticklabels([]) #Remove x-tic labels for the first frame\n",
    "    frame1.set_ylabel(\"Flux (mJy)\",fontsize = 13)\n",
    "    plt.plot(x,fit,color='green')\n",
    "    plt.grid()\n",
    "    \n",
    "#   initialize residual frame\n",
    "    frame2=fig1.add_axes((.1,.1,.8,.2),sharex = frame1) \n",
    "    matplotlibfunction(frame2,freq,(flux-starmodel2(freq,*ml_cfpars))/flux,fluxerr/flux,'green')\n",
    "    plt.axhline(0.0, color='purple', linestyle='dotted', lw=2)\n",
    "    frame2.set_xscale('log')\n",
    "    frame2.set_xlabel('Frequency (MHz)',fontsize=13)\n",
    "    plt.grid()\n",
    "    if savefigs:\n",
    "        try:\n",
    "            homedir = os.getcwd()\n",
    "            newdir = homedir+\"/\" +timescale+\"_\"+ technique\n",
    "            os.mkdir(newdir)\n",
    "            os.chdir(newdir)\n",
    "            plt.savefig(timescale + str(ids) +\".png\" )\n",
    "            os.chdir(homedir)\n",
    "        except:\n",
    "            os.chdir(newdir)\n",
    "            plt.savefig(timescale + str(ids) +\".png\" )\n",
    "            os.chdir(homedir)\n",
    "    storedfitvalueslist.append([ids,chisquare,dof])\n",
    "    plt.show() \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print storedfitvalueslist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do not add tgss or vlssr data to the fit for this singular source where shit hits the fan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weirdactingid = 7605\n",
    "for i in dfFlux.id.unique():\n",
    "    if i == str(weirdactingid):\n",
    "        specid = dfFlux[dfFlux.id == i]\n",
    "        ids = int(i)\n",
    "        freq = np.array(specid.nu.astype(float))\n",
    "        flux = np.array(specid.s_nu.astype(float))\n",
    "        fluxerr = np.array(specid.e.astype(float))\n",
    "        ids = int(i)\n",
    "        \n",
    "for i in range(len(storedfitvalueslist)):\n",
    "    if storedfitvalueslist[i][0]== weirdactingid:\n",
    "        print storedfitvalueslist[i]\n",
    "        del(storedfitvalueslist[i])\n",
    "        \n",
    "# Initialize figure\n",
    "fig1 = plt.figure(1,figsize=(10,10))\n",
    "fig1.suptitle(\"Plot of: \" + str(ids),fontsize=20)\n",
    "\n",
    "#add frame1\n",
    "frame1=fig1.add_axes((.1,.3,.8,.6))\n",
    "matplotlibfunction(frame1,freq,flux,fluxerr,\"blue\")\n",
    "\n",
    "\n",
    "# initialize x\n",
    "x = np.arange(1e1, 1e4, 1)\n",
    "try:\n",
    "    fit,chisquare,dof,ml_cfpars, ml_cfcovar = fitexistingdata(freq,flux,fluxerr,ids,x)\n",
    "\n",
    "except:\n",
    "    print \"for id: %i there is no good fit\" %ids\n",
    "    print freq\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "for j in FluxVar.instances:\n",
    "    if j.id == ids:\n",
    "\n",
    "        matplotlibfunction(frame1,j.database.freq,j.database.pflux[j.entry]*10**3,j.database.pfluxe[j.entry]*10**3,colour='blue')\n",
    "\n",
    "\n",
    "\n",
    "getSourceData2(frame1,int(ids),colour = 'red')\n",
    "\n",
    "# Initialize normal frame\n",
    "frame1.set_xscale('log')\n",
    "frame1.set_yscale('log')\n",
    "frame1.set_xticklabels([]) #Remove x-tic labels for the first frame\n",
    "frame1.set_ylabel(\"Flux (mJy)\",fontsize = 13)\n",
    "plt.plot(x,fit,color='green')\n",
    "plt.grid()\n",
    "\n",
    "# initialize residuals frame\n",
    "frame2=fig1.add_axes((.1,.1,.8,.2),sharex = frame1) \n",
    "matplotlibfunction(frame2,freq,(flux-starmodel2(freq,*ml_cfpars))/flux,fluxerr/flux,'green')\n",
    "plt.axhline(0.0, color='purple', linestyle='dotted', lw=2)\n",
    "frame2.set_xscale('log')\n",
    "frame2.set_xlabel('Frequency (MHz)',fontsize=13)\n",
    "plt.grid()\n",
    "\n",
    "storedfitvalueslist.append([ids,chisquare,dof])\n",
    "# if we wanna save we save\n",
    "if savefigs:\n",
    "    os.chdir(newdir)\n",
    "    plt.savefig(timescale + str(ids) +\".png\" )\n",
    "    os.chdir(homedir)\n",
    "plt.show()\n",
    "# make figure for and get the residuals\n",
    "\n",
    "# p = figure(title=\"residuals\", x_axis_label='freq (MHz)', y_axis_label='flux(mJy)',x_axis_type='log')\n",
    "\n",
    "\n",
    "# p = bokehplotfunction(p,freq,(flux-starmodel2(freq,*ml_cfpars))/flux,fluxerr/flux,'green')\n",
    "# p.line(x,0,color='purple',line_dash='dashed')\n",
    "\n",
    "# show(p)\n",
    "print \"__________________________________________________________________________________________\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Reg.file for quick comparison in original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.chdir(newdir)\n",
    "except:\n",
    "    homedir = os.getcwd()\n",
    "    newdir = homedir+\"/\" +timescale+ \"_\"+technique\n",
    "    os.chdir(newdir)\n",
    "\n",
    "wr.WriteReg(timescale,PosVar.instances,FluxVar.instances)\n",
    "os.chdir(homedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST MAKE IMAGES FOR TGSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsonwriter.WriteFluxJson(PosVar.instances,pos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsonFile = open(\"tgpos.json\",\"r\")\n",
    "# datalistjson = json.load(jsonFile)\n",
    "# n = len(datalistjson)\n",
    "# jsonFile.close()\n",
    "# try:\n",
    "#     os.remove('poslink.json')\n",
    "# except:\n",
    "#     print 'allready removed'\n",
    "# for i in range(n):\n",
    "#     !{sys.executable} -m scrapy runspider databasescrapper.py -o poslink.json\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download tgss fits images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# os.chdir(homedir)\n",
    "# try:\n",
    "#     os.rename(homedir+\"/poslink.json\", newdir+\"/poslink.json\")\n",
    "# except:\n",
    "#     pass\n",
    "# try:\n",
    "#     os.rename(homedir+\"/downloader.py\", newdir+\"/downloader.py\")\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "\n",
    "# os.chdir(newdir)\n",
    "# !{sys.executable} downloader.py\n",
    "\n",
    "# try:\n",
    "#     os.rename(newdir+\"/downloader.py\", homedir+\"/downloader.py\")\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "# os.chdir(homedir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert fits to png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# os.chdir(newdir)\n",
    "# imagez = glob.glob('*.fits')\n",
    "# # for i in imagez:\n",
    "# # #     print i\n",
    "# #     rpf.Imager(i)\n",
    "# for i in PosVar.instances:\n",
    "#     if i.dataname == 'tgss':\n",
    "#         a = str(i.id) +'tgss.fits'\n",
    "#         print i.ra\n",
    "#         print i.dec\n",
    "#         print i.id\n",
    "#         rpf.Imager(a,i.ra,i.dec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make cutouts vlssr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.chdir(homedir)\n",
    "os.chdir(newdir)\n",
    "\n",
    "for i in PosVar.instances:\n",
    "    if i.dataname == 'vlssr':\n",
    "\n",
    "        ct.GetCutout('../vlssrimage.fits',i.ra,i.dec,i.id,'vlssr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make cutouts tgss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.chdir(homedir)\n",
    "# os.rename('tgssimage.fits','/'+newdir+\"tgssimage.fits\")\n",
    "os.chdir(newdir)\n",
    "\n",
    "for i in PosVar.instances:\n",
    "    if i.dataname == 'tgss':\n",
    "\n",
    "        ct.GetCutout('../tgssimage.fits',i.ra,i.dec,i.id,'tgss')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make cutouts of own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.chdir(homedir)\n",
    "os.chdir(newdir)\n",
    "for i in PosVar.instances:\n",
    "    ct.GetCutout(i.url[0],i.ra,i.dec,i.id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot in Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print \"\\\\begin{longtable}{m{1cm}|m{6cm}|m{6cm}|m{6cm}} \\n\\\n",
    "# \\\\toprule \\nId & Data & TGSS (deg)& VLSSR  \\\\\\\\\\\\midrule \\n\\\\endfirsthead \\n\\\\toprule \\\n",
    "# \\nId & Data & TGSS (deg)& VLSSR   \\\\\\\\\\\\midrule \\n\\\\endhead \\n\\\\bottomrule \\n\\\\endfoot \\n\\\\bottomrule\"\n",
    "\n",
    "# # define caption for table\n",
    "# print \"\\\\caption{ Comparison position of candidates with different databases for the 1hr interval data} \\n\\\\endlastfoot\"\n",
    "\n",
    "# for i in samesieslist:\n",
    "#     a = \"\\\\raisebox{-\\\\totalheight}{\\\\includegraphics[width=6cm, height=6cm]{\"+technique+\"/\"+str(i[0])+\"cutout_own.png}}\"+\" & \"\n",
    "#     if i[1] == \"tgss\":\n",
    "#         b = (\"\\\\raisebox{-\\\\totalheight}{\\\\includegraphics[width=6cm, height=6cm]{\"+technique+\"/\"+str(i[0])+\"cutout_tgss.png}}\"+\" & - \\\\\\\\\")\n",
    "#     if i[1] ==\"vlssr\":\n",
    "#         b = (\"- & \\\\raisebox{-\\\\totalheight}{\\\\includegraphics[width=6cm, height=6cm]{\"+technique+\"/\"+str(i[0])+\"cutout_vlssr.png}} \\\\\\\\\")\n",
    "#     if i[1] == \"tgss \\\\& vlssr\":\n",
    "#         b = (\"\\\\raisebox{-\\\\totalheight}{\\\\includegraphics[width=6cm, height=6cm]{\"+technique+\"/\"+str(i[0])+\"cutout_tgss.png}}\"+\" \"+ \"&\"\\\n",
    "#                 + \" \"+ \"\\\\raisebox{-\\\\totalheight}{\\\\includegraphics[width=6cm, height=6cm]{\"+technique+\"/\"+str(i[0])+\"cutout_vlssr.png}} \\\\\\\\\")\n",
    "#     print latexHREF +str(i[0])+\"}\" + \"{%i}\" %i[0]+\" \"+ \"&\"+ \" \"+ a + b\n",
    "\n",
    "# print \"\\\\end{longtable}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  PRINT EVERYTHING IN LATEX IN ONE GO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "folder = technique+\"_\"+timescale+\"/\"\n",
    "\n",
    "# HERE WE PRINT THE NECESSARY PACKAGES\n",
    "print\"\"\"\n",
    "\\documentclass{article}\n",
    "\\usepackage[utf8]{inputenc}\n",
    "\\usepackage[utf8]{inputenc}\n",
    "\\usepackage{geometry}\n",
    "\\usepackage{booktabs}\n",
    "\\usepackage{array}\n",
    "\\usepackage{graphicx}% delete the demo option in your actual code\n",
    "\\usepackage{longtable}\n",
    "\\usepackage{pdflscape}\n",
    "\\usepackage{mathabx}\n",
    "\\usepackage{float}\n",
    "\\usepackage{multirow}\n",
    "\\usepackage{multicol}\n",
    "\\usepackage{bigstrut}\n",
    "\\usepackage{caption}\n",
    "\\usepackage{subcaption}\n",
    "\\usepackage{siunitx}\n",
    "\\usepackage{makecell}\n",
    "\\usepackage{textcomp}\n",
    "\\usepackage{titlesec}\n",
    "\\usepackage[para]{footmisc}\n",
    "\\usepackage[nottoc]{tocbibind}\n",
    "\\usepackage{hyperref}\n",
    "\\geometry{lmargin=.5cm,bmargin=0.5cm,rmargin=1.5cm}\n",
    "\\hypersetup{\n",
    "    colorlinks=true,\n",
    "    linkcolor=blue,\n",
    "    filecolor=magenta,      \n",
    "    urlcolor=cyan,\n",
    "}\n",
    "\\\\begin{document}\n",
    "\"\"\"\n",
    "# HERE WE PRINT THE NUMBER TABLES OF POSITION VARIATING SOURCES\n",
    "\n",
    "print \"\\\\section{Position variating Sources}\\\\label{possection:\"+technique+\":\"+timescale+\"}\\n\"\n",
    "samesieslist = []\n",
    "for i in range(len(PosVar.instances)):\n",
    "    samesie = False\n",
    "    \n",
    "#   create combined name if entry both in vlssr and tgss\n",
    "    for j in range(len(samesieslist)):\n",
    "        if PosVar.instances[i].id == samesieslist[j][0]:\n",
    "            samesieslist[j][1] = samesieslist[j][1] + \" \\& \" + PosVar.instances[i].database.name\n",
    "            samesie = True\n",
    "    if not samesie:\n",
    "        samesieslist.append([PosVar.instances[i].id,PosVar.instances[i].database.name,PosVar.instances[i]])\n",
    "    \n",
    "samesieslist = sorted(samesieslist, key=itemgetter(1))\n",
    "\n",
    "print \"\\\\begin{longtable}{c|c|c|c|c} \\n\\\n",
    "\\\\toprule \\nDatabase & Ra (deg) & Dec (deg)& Fk5 & Id/Link \\\\\\\\\\\\midrule \\n\\\\endfirsthead \\n\\\\toprule \\\n",
    "\\nDatabase & Ra (deg) & Dec (deg) & Fk5 &Id/Link  \\\\\\\\\\\\midrule \\n\\\\endhead \\n\\\\bottomrule \\n\\\\endfoot \\n\\\\bottomrule\"\n",
    "\n",
    "# define caption for table\n",
    "print \"\\\\caption{ Comparison position of candidates with TGSS and VLSSR for the\" +timescale+\" timescale data} \\n\\\\endlastfoot\\n\\\n",
    "\\\\label{\"+technique+\":\"+timescale+\":tablepos\"+\"}\\n\"\n",
    "\n",
    "lastname = None\n",
    "for i in samesieslist:\n",
    "    if i[1] != lastname and lastname:\n",
    "        print \"\\\\hline\"\n",
    "    print i[1] +\" \"+ \"&\"+ \" \"+ '%0.3f' %i[2].ra + \" \" + \"&\"+ \" \"+ \"%0.3f\" %(i[2].dec) +\\\n",
    "    \" \" + \"&\" + \" \" +i[2].fk5 +\" \" + '&' + \"  \"+ latexHREF +str(i[0])+\"}\" + \"{%i}\" %i[0] +\" \\\\\" + \"\\\\\"\n",
    "    \n",
    "    lastname = i[1]\n",
    "\n",
    "print \"\\\\end{longtable}\\n\"\n",
    "\n",
    "\n",
    "# HERE WE PRINT HE FIGURE TABLES\n",
    "print \"\\\\subsection{Images}\"\n",
    "\n",
    "print \"\\\\begin{longtable}{m{1cm}|m{6cm}|m{6cm}|m{6cm}} \\n\\\n",
    "\\\\toprule \\nId & Data & TGSS (deg)& VLSSR  \\\\\\\\\\\\midrule \\n\\\\endfirsthead \\n\\\\toprule \\\n",
    "\\nId & Data & TGSS (deg)& VLSSR   \\\\\\\\\\\\midrule \\n\\\\endhead \\n\\\\bottomrule \\n\\\\endfoot \\n\\\\bottomrule\"\n",
    "\n",
    "# define caption for table\n",
    "print \"\\\\caption{Images to compare the  position of our candidates with TGSS and VLSSR for the\" +timescale+\" timescale data} \\n\\\\endlastfoot\"\n",
    "print \"\\\\label{\"+technique+\":\"+timescale+\":tablepos_images\"+\"}\"\n",
    "for i in samesieslist:\n",
    "    a = \"\\\\raisebox{-\\\\totalheight}{\\\\includegraphics[width=6cm, height=6cm]{\"+folder+str(i[0])+\"cutout_own.png}}\"+\" & \"\n",
    "    if i[1] == \"tgss\":\n",
    "        b = (\"\\\\raisebox{-\\\\totalheight}{\\\\includegraphics[width=6cm, height=6cm]{\"+folder+str(i[0])+\"cutout_tgss.png}}\"+\" & - \\\\\\\\\")\n",
    "    if i[1] ==\"vlssr\":\n",
    "        b = (\"- & \\\\raisebox{-\\\\totalheight}{\\\\includegraphics[width=6cm, height=6cm]{\"+folder+str(i[0])+\"cutout_vlssr.png}} \\\\\\\\\")\n",
    "    if i[1] == \"tgss \\\\& vlssr\":\n",
    "        b = (\"\\\\raisebox{-\\\\totalheight}{\\\\includegraphics[width=6cm, height=6cm]{\"+folder+str(i[0])+\"cutout_tgss.png}}\"+\" \"+ \"&\"\\\n",
    "                + \" \"+ \"\\\\raisebox{-\\\\totalheight}{\\\\includegraphics[width=6cm, height=6cm]{\"+folder+str(i[0])+\"cutout_vlssr.png}} \\\\\\\\\")\n",
    "    print latexHREF +str(i[0])+\"}\" + \"{%i}\" %i[0]+\" \"+ \"&\"+ \" \"+ a + b\n",
    "\n",
    "print \"\\\\end{longtable}\\n\"\n",
    "\n",
    "# HERE WE PRINT THE NUMBER TABLES OF FLUX VARIATING SOURCES\n",
    "\n",
    "print \"\\\\begin{landscape}\\n\"\n",
    "print \"\\\\section{Fluxvariating Sources}\\\\label{varsection:\"+technique+\":\"+timescale+\"}\\n\"\n",
    "# SEPERATE DATABASES\n",
    "databaselist = ['tgss','vlssr']\n",
    "\n",
    "samesieslist = []\n",
    "for i in range(len(FluxVar.instances)):\n",
    "    samesie = False\n",
    "    tempdict = {'tgss':None,'vlssr':None,'id':None,'ra':None,'dec':None,'fk5':None}\n",
    "    \n",
    "#   create combined name if entry both in vlssr and tgss\n",
    "    for j in range(len(samesieslist)):\n",
    "        if FluxVar.instances[i].id == samesieslist[j]['id']:\n",
    "#             samesieslist[j].append(FluxVar.instances[i])\n",
    "            samesieslist[j][FluxVar.instances[i].database.name] = FluxVar.instances[i]\n",
    "          \n",
    "            samesie = True\n",
    "    if not samesie:\n",
    "        tempdict[FluxVar.instances[i].database.name] = FluxVar.instances[i]\n",
    "        tempdict['id'] = FluxVar.instances[i].id\n",
    "        tempdict['ra'] = FluxVar.instances[i].ra\n",
    "        tempdict['dec'] = FluxVar.instances[i].dec\n",
    "        tempdict['fk5'] = FluxVar.instances[i].fk5\n",
    "        samesieslist.append(tempdict)\n",
    "\n",
    "\n",
    "print \"\\\\begin{longtable}{c|c|c|c|c|c|c|c|c|c|c|c} \\n\\\n",
    "\\\\toprule \\nFlux147.5MHz (Jy) & Tgss (Jy) & Flux74MHz (Jy)& Vlssr (Jy) & fk5 & Ra (deg) & Dec (deg) &tggsRa&tgssDec&\\\n",
    "vlssrRa&vlssrDec& Id/Link \\\\\\\\\\\\midrule \\n\\\\endfirsthead \\n\\\\toprule \\\n",
    "\\nFlux147.5MHz (Jy) & Tgss (Jy) & Flux74MHz (Jy)& Vlssr (Jy)& fk5 & Ra (deg) & Dec (deg)  &tggsRa&tgssDec&\\\n",
    "vlssrRa&vlssrDec& Id/Link \\\\\\\\\\\\midrule \\n\\\\endhead \\n\\\\bottomrule \\n\\\\endfoot \\n\\\\bottomrule\"\n",
    "\n",
    "# define caption for table\n",
    "print \"\\\\caption{ Comparison of fluxes that vary from catalogues TGSS and VLSSR for \" + timescale+\" images\"+\"} \\n\\\\endlastfoot\"\n",
    "print \"\\\\label{\"+technique+\":\"+timescale+\":tablevar\"+\"}\"\n",
    "lastname = None\n",
    "# samesieslist = sorted(samesieslist, key=lambda e: (e['ra'], e['dec']))\n",
    "for i in samesieslist:\n",
    "    a= ''\n",
    "    b=''\n",
    "    for j in(databaselist):\n",
    "        try: \n",
    "            a+= str(np.around(i[j].pflux*i[j].scale,3)) + \" \" + \"&\"+ \" \"\n",
    "        except:\n",
    "            a += \"-\" +\" \"+ \"&\"+ \" \"\n",
    "        try:\n",
    "            a+= str(np.around(i[j].database.pflux[i[j].entry],3)) + \" \" + \"&\"+ \" \"   \n",
    "        except:\n",
    "            a += \"-\" +\" \"+ \"&\"+ \" \"\n",
    "            \n",
    "        try: \n",
    "            b+= '%0.3f' %(i[j].database.ra[i[j].entry])+\" \"+\"&\"+\" \"\n",
    "            b+= '%0.3f' %(i[j].database.dec[i[j].entry])+\" \"+\"&\"+\" \"\n",
    "        except:\n",
    "            b+= \"-\" + \" \" + \"&\"+\" \" \"-\" + \" \" + \"&\"+\" \"\n",
    "        \n",
    "    a +=i['fk5'] + \" \" + '&' + \"  \"+ '%0.3f' %i['ra'] + \" \" + \"&\"+ \" \"+ \"%0.3f\" %i['dec'] \\\n",
    "    + \" \" + '&' + \"  \"+ b+ latexHREF +str(i[\"id\"])+\"}\" + \"{%i}\" %i['id'] +\"  \\\\\" + \"\\\\\"\n",
    "    print a\n",
    "\n",
    "print \"\\\\end{longtable}\\n\"\n",
    "print \"\\\\end{landscape}\\n\"\n",
    "\n",
    "## HERE ARE THE PLOTS:\n",
    "print \"\\\\subsection{Fits}\"\n",
    "\n",
    "i = 0\n",
    "while i < len(storedfitvalueslist):\n",
    "    print \"\\\n",
    "\\\\begin{figure}[H]\\n\\\n",
    "    \\\\centering\\n\\\n",
    "    \\\\begin{minipage}{.5\\\\textwidth}\\n\\\n",
    "        \\\\centering\\n\\\n",
    "        \\\\includegraphics[scale = 0.35]{\"+folder+timescale+str(storedfitvalueslist[i][0])+\".png}\\n\\\n",
    "        \\\\captionsetup{labelformat=empty}\\n\\\n",
    "        \\\\caption{Plot of source: \"+latexHREF +str(storedfitvalueslist[i][0])+\"}\"+\"{%i}\" %storedfitvalueslist[i][0]\\\n",
    "    +\",\\\\\\\\with a goodness of fit of: %0.2f and a dof of: %i\"\\\n",
    "    %(storedfitvalueslist[i][1],storedfitvalueslist[i][2]) +\"}\\n\\\n",
    "        \\\\addtocounter{figure}{-1}\\n\\\n",
    "        \\\\label{\"+technique+\":\"+timescale+\":\"+str(storedfitvalueslist[i][0]) +\":plot}\\n\\\n",
    "    \\\\end{minipage}%\\n\\\n",
    "    \\\\begin{minipage}{0.5\\\\textwidth}\\n\\\n",
    "        \\\\centering\\n\"\n",
    "    try:\n",
    "        print\"\\\n",
    "        \\\\includegraphics[scale = 0.35]{\"+folder+timescale+str(storedfitvalueslist[i+1][0])+\".png}\\n\\\n",
    "        \\\\captionsetup{labelformat=empty}\\n\\\n",
    "        \\\\caption{Plot of source: \"+latexHREF +str(storedfitvalueslist[i+1][0])+\"}\"+\"{%i}\" %storedfitvalueslist[i+1][0]\\\n",
    "    +\",\\\\\\\\with a goodness of fit of: \"+str(storedfitvalueslist[i+1][1].round(2))+\\\n",
    "    \" and a dof of: \"+str(storedfitvalueslist[i+1][2])+\"\"\\\n",
    "    +\"}\\n\\\n",
    "    \\\\addtocounter{figure}{-1}\\n\\\n",
    "    \\\\label{\"+technique+\":\"+timescale+\":\"+str(storedfitvalueslist[i+1][0]) +\":plot}\\n\\\n",
    "    \\\\end{minipage}\\n\\\n",
    "\\\\end{figure}\"\n",
    "    except:\n",
    "        print \"--\\n\\\n",
    "        \\\\end{minipage}\\n\\\n",
    "\\\\end{figure}\"\n",
    "    if (i+2)%8==0 and i !=0:\n",
    "        if i+2 ==8:\n",
    "            pass\n",
    "        else:\n",
    "            print \"\\\\newpage\"\n",
    "    i+=2\n",
    "    \n",
    "# ENDING THE DOCUMENT\n",
    "\n",
    "print \"\\\\end{document}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# i = 0\n",
    "# while i < len(storedfitvalueslist):\n",
    "#     print \"\\\n",
    "# \\\\begin{figure}[H]\\n\\\n",
    "#     \\\\centering\\n\\\n",
    "#     \\\\begin{minipage}{.5\\\\textwidth}\\n\\\n",
    "#         \\\\centering\\n\\\n",
    "#         \\\\includegraphics[scale = 0.35]{\"+technique+\"/\"+timescale+str(storedfitvalueslist[i][0])+\".png}\\n\\\n",
    "#         \\\\captionsetup{labelformat=empty}\\n\\\n",
    "#         \\\\caption{\\\\textbf{a:} Plot of \"+latexHREF +str(storedfitvalueslist[i][0])+\"}\"+\"{%i}\" %storedfitvalueslist[i][0]\\\n",
    "#     +\",\\\\\\\\with a goodness of fit of: %0.2f and a dof of: %i\"\\\n",
    "#     %(storedfitvalueslist[i][1],storedfitvalueslist[i][2]) +\"}\\n\\\n",
    "#         \\\\addtocounter{figure}{-1}\\n\\\n",
    "#         \\\\label{\"+technique+\":\"+timescale+\":\"+str(storedfitvalueslist[i][0]) +\":plot}\\n\\\n",
    "#     \\\\end{minipage}%\\n\\\n",
    "#     \\\\begin{minipage}{0.5\\\\textwidth}\\n\\\n",
    "#         \\\\centering\\n\\\n",
    "#         \\\\includegraphics[scale = 0.35]{\"+technique+\"/\"+timescale+str(storedfitvalueslist[i][0])+\".png}\\n\\\n",
    "#         \\\\captionsetup{labelformat=empty}\\n\\\n",
    "#         \\\\caption{\\\\textbf{b:} Plot of \"+latexHREF +str(storedfitvalueslist[i+1][0])+\"}\"+\"{%i}\" %storedfitvalueslist[i+1][0]\\\n",
    "#     +\",\\\\\\\\with a goodness of fit of: \"+str(storedfitvalueslist[i+1][1].round(2))+\\\n",
    "#     \" and a dof of: \"+str(storedfitvalueslist[i+1][2])+\"\"\\\n",
    "#     +\"}\\n\\\n",
    "#     \\\\addtocounter{figure}{-1}\\n\\\n",
    "#     \\\\label{\"+technique+\":\"+timescale+\":\"+str(storedfitvalueslist[i+1][0]) +\":plot}\\n\\\n",
    "#     \\\\end{minipage}\\n\\\n",
    "# \\\\end{figure}\"\n",
    "#     if (i+2)%8==0 and i !=0:\n",
    "#         if i+2 ==8:\n",
    "#             pass\n",
    "#         else:\n",
    "#             print \"\\\\newpage\"\n",
    "#     i+=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
